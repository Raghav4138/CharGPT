{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Char-GPT Implementation from Scratch"
      ],
      "metadata": {
        "id": "XxN2VC0bAyS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3gSD88oAxSA",
        "outputId": "d0b2f5e1-d48f-4170-cb36-79195f5cf81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-30 17:44:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-10-30 17:44:53 (16.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the train data text\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we read the input data, which is tiny-shakespheare data\n",
        "with open('input.txt', 'r', encoding='utf-8') as input_file:\n",
        "    text = input_file.read()"
      ],
      "metadata": {
        "id": "NLf5UXXoA-2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we interpret the data\n",
        "print(f\"The length of the Dataset is: {len(text):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqj1rsWXBoDe",
        "outputId": "747a4fa1-4f82-4ea3-f9ab-fb78ae24b220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the Dataset is: 1,115,394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at the first 1000 chars in the data\n",
        "print(\"Tiny-Shakespear Dataset\")\n",
        "print(\"----------------------------------------------\\n\")\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv4DX0LfCHTx",
        "outputId": "1569d28d-695e-4226-b01a-2e5c4d353b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny-Shakespear Dataset\n",
            "----------------------------------------------\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let us now check the unique characters in the text\n",
        "unique_chars = sorted((set(text)))\n",
        "vocab_size = len(unique_chars)\n",
        "\n",
        "print(f\"Vocabulary Size: {vocab_size}\\n\")\n",
        "print(\"The unique characters in the vocabulary are:\")\n",
        "print(''.join(unique_chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTeQqTIQCqZS",
        "outputId": "1f8a5105-6c7d-46aa-d1d7-d4da9aa136dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 65\n",
            "\n",
            "The unique characters in the vocabulary are:\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from chars to index and index to chars\n",
        "stoi = {s:i for i,s in enumerate(unique_chars)}\n",
        "itos = {i:s for i,s in enumerate(unique_chars)}\n",
        "\n",
        "def encode(text):\n",
        "    return [stoi[c] for c in text]\n",
        "\n",
        "def decode(indexes):\n",
        "    return \"\".join([itos[i] for i in indexes])\n",
        "\n",
        "print(encode('raghav'))\n",
        "print(decode(encode('raghav')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkxxtXURDhqV",
        "outputId": "f81466b2-c4da-47e3-c5c9-9c0d6ad0482d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56, 39, 45, 46, 39, 60]\n",
            "raghav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets apply encode to whole data text and form our data as PyTorch Tensors\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(f\"Data Shpae: {len(data):,} \\n Data Type: {data.dtype}\")\n",
        "print(\"A sample of data\")\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2x9g6_DFdHj",
        "outputId": "00abb4c7-6786-4bef-aba0-887d47835e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shpae: 1,115,394 \n",
            " Data Type: torch.int64\n",
            "A sample of data\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "train_size = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:]"
      ],
      "metadata": {
        "id": "HReJsiM0GiGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the maximum context length for predictions?\n",
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVUcQ-I4GtcH",
        "outputId": "6e60aaa6-9f66-493b-daa0-87648280f113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_data[:block_size]\n",
        "y_train = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x_train[:t + 1]\n",
        "    output = y_train[t]\n",
        "    print(f\"For the context/input {context}, we need output/generated word {output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ASksZhuHLke",
        "outputId": "0b709f8b-e987-4a21-c206-2840735d5f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the context/input tensor([18]), we need output/generated word 47\n",
            "For the context/input tensor([18, 47]), we need output/generated word 56\n",
            "For the context/input tensor([18, 47, 56]), we need output/generated word 57\n",
            "For the context/input tensor([18, 47, 56, 57]), we need output/generated word 58\n",
            "For the context/input tensor([18, 47, 56, 57, 58]), we need output/generated word 1\n",
            "For the context/input tensor([18, 47, 56, 57, 58,  1]), we need output/generated word 15\n",
            "For the context/input tensor([18, 47, 56, 57, 58,  1, 15]), we need output/generated word 47\n",
            "For the context/input tensor([18, 47, 56, 57, 58,  1, 15, 47]), we need output/generated word 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 2810240409 # this is the time when i was writing this code (DDMMYYhhmm) so 28-10-2024, 04:09 in the morning\n",
        "torch.manual_seed(seed_val)\n",
        "\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    random_indexes = torch.randint(low=0, high=len(data)-block_size, size=(batch_size, ))\n",
        "    x_b = torch.stack([data[i:i+block_size] for i in random_indexes])\n",
        "    y_b = torch.stack([data[i+1:i+block_size+1] for i in random_indexes])\n",
        "    return x_b , y_b\n",
        "\n",
        "x_b, y_b = get_batch(\"train\")\n",
        "\n",
        "# Visualize the batche (x and y)\n",
        "print(f'Input Shape: {x_b.shape}')\n",
        "print(x_b)\n",
        "print(f'\\nOutput Shape: {y_b.shape}')\n",
        "print(y_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEpi8JR4ID5A",
        "outputId": "f70963ed-6ef5-40a0-9e64-91b57e6faeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([4, 8])\n",
            "tensor([[ 0, 31, 46, 39, 50, 50,  1, 40],\n",
            "        [15, 17, 10,  0, 21,  1, 46, 43],\n",
            "        [ 1, 54, 56, 43, 57, 57, 43, 57],\n",
            "        [44,  1, 41, 59, 56, 57, 43, 42]])\n",
            "\n",
            "Output Shape: torch.Size([4, 8])\n",
            "tensor([[31, 46, 39, 50, 50,  1, 40, 43],\n",
            "        [17, 10,  0, 21,  1, 46, 43, 39],\n",
            "        [54, 56, 43, 57, 57, 43, 57,  1],\n",
            "        [ 1, 41, 59, 56, 57, 43, 42,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(batch_size):\n",
        "    print(f'Batch Number - {b}')\n",
        "    for t in range(block_size):\n",
        "        context = x_b[b][:t + 1]\n",
        "        output = y_b[b][t]\n",
        "        print(f\"For the context {context}, we need output {output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieekiN_0LseL",
        "outputId": "39424414-adee-4202-8506-750b8cdc85ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Number - 0\n",
            "For the context tensor([0]), we need output 31\n",
            "For the context tensor([ 0, 31]), we need output 46\n",
            "For the context tensor([ 0, 31, 46]), we need output 39\n",
            "For the context tensor([ 0, 31, 46, 39]), we need output 50\n",
            "For the context tensor([ 0, 31, 46, 39, 50]), we need output 50\n",
            "For the context tensor([ 0, 31, 46, 39, 50, 50]), we need output 1\n",
            "For the context tensor([ 0, 31, 46, 39, 50, 50,  1]), we need output 40\n",
            "For the context tensor([ 0, 31, 46, 39, 50, 50,  1, 40]), we need output 43\n",
            "Batch Number - 1\n",
            "For the context tensor([15]), we need output 17\n",
            "For the context tensor([15, 17]), we need output 10\n",
            "For the context tensor([15, 17, 10]), we need output 0\n",
            "For the context tensor([15, 17, 10,  0]), we need output 21\n",
            "For the context tensor([15, 17, 10,  0, 21]), we need output 1\n",
            "For the context tensor([15, 17, 10,  0, 21,  1]), we need output 46\n",
            "For the context tensor([15, 17, 10,  0, 21,  1, 46]), we need output 43\n",
            "For the context tensor([15, 17, 10,  0, 21,  1, 46, 43]), we need output 39\n",
            "Batch Number - 2\n",
            "For the context tensor([1]), we need output 54\n",
            "For the context tensor([ 1, 54]), we need output 56\n",
            "For the context tensor([ 1, 54, 56]), we need output 43\n",
            "For the context tensor([ 1, 54, 56, 43]), we need output 57\n",
            "For the context tensor([ 1, 54, 56, 43, 57]), we need output 57\n",
            "For the context tensor([ 1, 54, 56, 43, 57, 57]), we need output 43\n",
            "For the context tensor([ 1, 54, 56, 43, 57, 57, 43]), we need output 57\n",
            "For the context tensor([ 1, 54, 56, 43, 57, 57, 43, 57]), we need output 1\n",
            "Batch Number - 3\n",
            "For the context tensor([44]), we need output 1\n",
            "For the context tensor([44,  1]), we need output 41\n",
            "For the context tensor([44,  1, 41]), we need output 59\n",
            "For the context tensor([44,  1, 41, 59]), we need output 56\n",
            "For the context tensor([44,  1, 41, 59, 56]), we need output 57\n",
            "For the context tensor([44,  1, 41, 59, 56, 57]), we need output 43\n",
            "For the context tensor([44,  1, 41, 59, 56, 57, 43]), we need output 42\n",
            "For the context tensor([44,  1, 41, 59, 56, 57, 43, 42]), we need output 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each bactch, our model will have batch_size * block_size examples to train on, i.e,  <br>\n",
        "4 * 8 = 32 training examples"
      ],
      "metadata": {
        "id": "5o2MzoXNNKHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_b # Input to our model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZVweBJMMxeR",
        "outputId": "bb66e055-7ba3-4c0d-e185-49e98b6ca8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0, 31, 46, 39, 50, 50,  1, 40],\n",
              "        [15, 17, 10,  0, 21,  1, 46, 43],\n",
              "        [ 1, 54, 56, 43, 57, 57, 43, 57],\n",
              "        [44,  1, 41, 59, 56, 57, 43, 42]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a BaseLine LM"
      ],
      "metadata": {
        "id": "oXeghf5mSKa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocal_size):\n",
        "        super().__init__()\n",
        "        self.model_embeddings = nn.Embedding(vocal_size, vocal_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) sized tensors where B is batch_dim and\n",
        "        # T is the time dimention\n",
        "        logits = self.model_embeddings(idx) # shape of this is (B,T,vocab_size)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_tokens):\n",
        "        for _ in range(max_tokens):\n",
        "            logits, _ = self(idx) # (B, T, C)\n",
        "            logits = logits[:, -1, :] # focus on last timestep (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_idx), dim = 1) # (B, T + 1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "zGhdGk0bPu2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLM(vocab_size)\n",
        "logits, loss = m(x_b, y_b)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "starting_index = torch.zeros((1,1), dtype=torch.long)\n",
        "generated_text = m.generate(idx=starting_index, max_tokens=100)\n",
        "print(decode(generated_text.tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YifjUZa_P9HS",
        "outputId": "d666dc8c-560c-4845-cf34-af5f65c92f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.4184, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "ZJE&;ChYFMQy'JX hXChATmVPUuweVLoX r-?U y,AUZwk.SLvKPUXDL CiBvlDRIhApZYKtpemgSLZC.ea.iLEaNafoqU:UqP;j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = x_b[0][2]\n",
        "token = itos[token_id.item()]\n",
        "\n",
        "print(f\"\\nEmbeddings for the token: {token} with token id {token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPfQNFvSaNLY",
        "outputId": "7ac8b99b-8ea9-4447-e40a-69fe922a610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embeddings for the token: h with token id 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch Optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "UT6bZR6aTNir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for iter in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go3dR5k3fcyf",
        "outputId": "d2f5c3ea-11d7-4549-cc6f-7a22b5196cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.603006601333618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starting_index = torch.zeros((1,1), dtype=torch.long)\n",
        "generated_text = m.generate(idx=starting_index, max_tokens=300)\n",
        "print(decode(generated_text.tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhy4Ab9UgGGk",
        "outputId": "fafe2e7f-04f3-4bee-a39a-2530a3d9596a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Shag.\n",
            "Q:\n",
            "pereatFitoomatis.\n",
            "Whimat lomt tr:\n",
            "RYo mby may nd,DWjowa ro Whisend stha ug burth; ILashay thand ARI ndghaggimang dmoQWhe thare o s mbym h o stciombloous?\n",
            "LELE:\n",
            "\n",
            "NIONG homak ts on de f send belps y htVI lof ne sscitediriarve aturerthy sthend tVed I:\n",
            "NGamet, vis d thealoule m'ThaveFiakne o'd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_seed = 3010242318\n",
        "torch.manual_seed(new_seed)"
      ],
      "metadata": {
        "id": "yjeSkJfcgQvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b56bf50-33d5-4653-c6ce-1a582ec05fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cedda578510>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjIra-TiiwIu",
        "outputId": "7388696a-10ee-4d64-d83b-0b439cb4bf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[0., 9.],\n",
            "        [4., 0.],\n",
            "        [6., 0.]])\n",
            "--\n",
            "c=\n",
            "tensor([[0.0000, 9.0000],\n",
            "        [2.0000, 4.5000],\n",
            "        [3.3333, 3.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b][:t+1]\n",
        "        xbow[b][t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "wCjctO77jLg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# method 2: Matrix Multiplication\n",
        "weights = torch.tril(torch.ones((T, T)))\n",
        "weights = weights / weights.sum(1, keepdim=True)\n",
        "xbow2 = weights @ x   # T*T @ B*T*C -> B*T*C\n",
        "\n",
        "torch.allclose(xbow2, xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAPPilnPkUKI",
        "outputId": "30bf15fc-8721-44d0-a9b9-c091c9c1648b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# method 3: Using Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "\n",
        "torch.allclose(xbow3, xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQIeij9ckhnN",
        "outputId": "d29cdcc6-6f11-4dea-f412-604326ed6fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self - Attention Mechanism\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C) # (B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)  # (C, H)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, H)\n",
        "q = query(x) # (B, T, H)\n",
        "v = value(x) # (B, T, H)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) --> (B, T, T)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "out = wei @ v # (B, T, T) @ (B, T, H) --> (B, T, H)\n",
        "print(out.shape)\n",
        "print(out[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOJ-Ca2flpJJ",
        "outputId": "50566ebd-aca5-49a8-8fee-05afb6440e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 16])\n",
            "tensor([[ 0.4408, -0.0845, -0.2914, -0.0040, -0.1214, -0.0151, -0.0061,  0.0641,\n",
            "         -0.5656, -0.3850,  0.1446,  0.2683,  0.3358,  0.2524,  0.0348, -0.0530],\n",
            "        [ 0.2834,  0.0955,  0.0221,  0.2278,  0.1417, -0.1201, -0.0363, -0.0793,\n",
            "          0.0639,  0.0893,  0.1220,  0.1180,  0.1415,  0.0967,  0.2284,  0.1327],\n",
            "        [ 0.2286,  0.1927,  0.1720,  0.2678,  0.3195, -0.0509,  0.0742, -0.1928,\n",
            "          0.3251,  0.3427,  0.1004,  0.0201, -0.0050,  0.0344,  0.3790,  0.0696],\n",
            "        [ 0.2327,  0.1634,  0.1187, -0.1020,  0.2671,  0.1542,  0.3511, -0.1045,\n",
            "          0.1681,  0.2750,  0.0609,  0.1405, -0.1215,  0.0770,  0.3897, -0.2454],\n",
            "        [-0.0530, -0.0823,  0.2314, -0.4032, -0.2570, -0.3784,  0.2028,  0.5928,\n",
            "          0.7820,  0.2318, -0.1211,  0.8496, -0.1356,  0.0364, -0.1285,  0.3486],\n",
            "        [-0.4204, -0.4193,  0.1771, -0.6056,  0.1721, -0.3711, -0.1882,  0.1966,\n",
            "          0.3316,  0.3388, -0.3928, -0.3039, -0.5216, -0.0107,  0.3781,  0.0348],\n",
            "        [ 0.3345, -0.1580, -0.1065,  0.0011, -0.0366, -0.0366,  0.1666,  0.0541,\n",
            "         -0.0801, -0.0524,  0.0218,  0.3589,  0.1681,  0.2442, -0.0169, -0.0205],\n",
            "        [ 0.3022, -0.2317,  0.0224, -0.0453,  0.2203,  0.1338,  0.5626, -0.2624,\n",
            "          0.2831,  0.4388, -0.0326,  0.3839, -0.0384,  0.3283, -0.0726, -0.2906]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "vuV7GmXPuZLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(32, 100)\n",
        "layer_norm = LayerNorm1d(100)\n",
        "x = layer_norm(x)\n",
        "print(x[0, :].mean(), x[0, :].std())\n",
        "print(x[:, 0].mean(), x[:, 0].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iycl4kKFHry",
        "outputId": "f9ef2146-9797-4f6e-a98e-6c5eecb83ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.3113e-08) tensor(1.0000)\n",
            "tensor(-0.3759) tensor(1.0856)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mPdKmN1FMtM",
        "outputId": "d2e221cc-3945-4270-e512-f5f58e37e1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0379, -1.8235,  0.5097,  ...,  0.8475, -1.5947,  0.7036],\n",
              "        [-2.2047,  0.8130, -1.9337,  ..., -0.4588, -0.1052, -0.3987],\n",
              "        [-0.5901, -0.2190, -0.0819,  ..., -0.3653,  0.3196, -0.3808],\n",
              "        ...,\n",
              "        [-1.0341,  0.9762,  0.3826,  ...,  1.6429,  0.1085,  0.4423],\n",
              "        [-0.2063,  1.5523,  0.2752,  ..., -0.1825,  0.2933,  0.3834],\n",
              "        [ 0.4230, -1.2092, -0.0720,  ...,  0.9585, -0.6663, -1.0341]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nfIG7AmFNcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}